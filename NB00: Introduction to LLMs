{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc965c69",
   "metadata": {},
   "source": [
    "# Chapter 0: Basic concepts required to understand LLM \n",
    "\n",
    "Before you start the notebook, I would like to clarify that this notebook serves more as a refresher of background conecpts required to understand LLMs. The notebook doesn't delve deeper into everything NLP, rather focussed on limited conecpts at a bird's eye level. \n",
    "\n",
    "If you want to dive deeper into NLP, here are some of resources I recommend:\n",
    "\n",
    "That being said, let's dive in deeper and set the foundations right before our chapter 1 that introduces LLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cea30",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing?\n",
    "\n",
    "\n",
    "**Natural language processing (NLP)** is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "\n",
    "Basically NLP is the way of communicating with the computers in natural language used by humans. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5529691",
   "metadata": {},
   "source": [
    "## Approaches in NLP\n",
    "\n",
    "The different approaches used to solve NLP problems commonly fall\n",
    "into three categories: \n",
    "\n",
    "<ul> 1. Heuristics </ul>\n",
    "<ul> 2. Machine learning </ul>\n",
    "<ul> 3. Deep learning </ul>\n",
    "\n",
    "## 1. Heuristics based NLP\n",
    "\n",
    "Similar to other early AI systems, early attempts at designing NLP systems were based on building rules for the task at hand. \n",
    "\n",
    "This required that the developers had some expertise in the domain to formulate rules that could be incorporated into a program. Such systems also required resources like dictionaries and thesauruses,typically compiled and digitized over a period of time. Eaxmples of such heuristics based NLP include Lexicon based sentiment analysis, regex, and context-free grammar. \n",
    "\n",
    "\n",
    "## 2. Machine Lerning based NLP\n",
    "\n",
    "Machine Learning for NLPMachine learning techniques are applied to textual data just as they’re used on other forms of data, such as images, speech, and structured data. \n",
    "\n",
    "Supervised machine learning techniques such as classification and regression methods are heavily used for various NLP tasks. As an example, an NLP classification task would be to classify news articles into a set of news topics like sports or politics. \n",
    "\n",
    "On the other hand, regression techniques, which give a numeric prediction, can be used to estimate the price of a stock based on processing the social media discussion about that stock. \n",
    "\n",
    "Similarly, unsupervised clustering algorithms can be used to club together text documents.\n",
    "\n",
    "Some algorithms to be used:\n",
    "\n",
    "<ul> 1. Naive Bayes Classifier </ul>\n",
    "<ul> 2. Support Vector Machine </ul>\n",
    "<ul> 3. Hidden Markov model </ul>\n",
    "\n",
    "## 3. Deep Learning for NLP\n",
    "\n",
    "In the last few years, we have seen a huge surge in using neural networks to deal with complex, unstructured data. Language is inherently complex and  unstructured.\n",
    "\n",
    "Therefore, we need models with better representation and learning\n",
    "capability to understand and solve language tasks. \n",
    "\n",
    "Few popular deep neural network architectures that can be used:\n",
    "\n",
    "<ul> 1. RNN </ul>\n",
    "<ul> 2. LSTM </ul>\n",
    "<ul> 3. CNN </ul>\n",
    "\n",
    "## Transformers: Talk of the town\n",
    "\n",
    "The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. The Transformer was proposed in the paper \"Attention Is All You Need\". It is recommended reading for anyone interested in NLP.\n",
    "\n",
    "They model the textual context but not in a sequential manner. Given a word in the input, it prefers to look at all the words around it (known as self  attention) and represent each word with respect to its context. For example, the word “bank” can have different meanings depending on the context in which it appears. If the context talks about finance, then “bank” probably denotes a financial institution. \n",
    "\n",
    "On the other hand, if the context mentions a river, then it probably indicates a bank of the river. Transformers can model such context and hence have been used heavily in NLP tasks due to this higher representation capacity as compared to other deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf736d9",
   "metadata": {},
   "source": [
    "## What is a language model?\n",
    "\n",
    "The course touches on Large Language Models, before we jump into large language model, it's important to understand what is a language model?\n",
    "\n",
    "**From Wikipedia**: A language model is a probabilistic model of a natural language that can generate probabilities of a series of words, based on text corpora in one or multiple languages it was trained on. It that made your head spin, don't worry! I am gonna simplify it for you. \n",
    "\n",
    "In any language, we have vocabulary and grammar, that help us communicate effectively and clearly. Those rules aren't clear to computers, as they understand only numbers, so they convert everything into numbers and create menaingful sentences using probability. Let's look at an example:\n",
    "\n",
    "I had coffee at coffeeshop -- looks good, makes sense \\\n",
    "I had wine at coffeeshop -- possible, but the above one makes more sense \\\n",
    "Wine had coffee at coffeeshop -- what?\n",
    "\n",
    "Now, machine will assign probability to each of these series of words after being trained on plethora of english language based data. It will assign higher probability to first statement, slightly lower for the second, and very low for the third one. \n",
    "\n",
    "p(I, had, coffee, at, coffeeshop) = 0.015 \\\n",
    "p(I, had, wine, at, coffeeshop) = 0.02 \\\n",
    "p(Wine, had, coffee, at, coffeeshop) = 0.00001\n",
    "\n",
    "Using language model, we can perfrom a hosts of tasks like speech recognition, handwriting recognition, machine translation, informational retrieval and natural language generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2d3aa",
   "metadata": {},
   "source": [
    "## How is the probability calculated?\n",
    "\n",
    "This is done using auto-regressive model, which forms the backbone of feed-forward neural network\n",
    "\n",
    "The probability here is calculated based the chain rule of probability:\n",
    "\n",
    "$ p(x_{1:k}) = p(x_{1} * p(x_{2}p(x_{1}) * p(x_{3}| x_{2},x_{1}) ........ p(x_{k}| x_{1:k-1})$\n",
    "\n",
    "For our example, it will translate to:\n",
    "\n",
    "p(I, had, coffee, at, coffeeshop) = p(I) * \\\n",
    "                                    p(had|I) * p(coffee| I, had)*\n",
    "                                    p(at | I, had, coffee) * \\\n",
    "                                    p( coffeeshop | I, had, coffee, at)\n",
    "                                    \n",
    "Does that look computationally expensive? It is! \n",
    "\n",
    "While now we have deep learning algorithms that can compute this in the form (with feed-forward neural networks mentioned above), a more computationally efficient method has been used: N-Grams model\n",
    "\n",
    "**N-Grams model**\n",
    "\n",
    "In an n-gram model, the prediction of a token $x_{i}$ only depends on the last n−1 characters $x_{i−(n−1):i−1}$ not on the whole corpus of k tokens, as done previously. The probability now would becomes:\n",
    "\n",
    "$p(x_{i∣x1:i−1})=p(x_{i}∣x_{i−(n−1):i−1})$\n",
    "\n",
    "In our example, for a 2 gram model, \n",
    "\n",
    "p(I, had, coffee, at, coffeeshop) = p(I)\n",
    "\n",
    "p(I, had, coffee, at, coffeeshop) = p(I) * \\\n",
    "                                    p(had|I) * p(coffee| I)*\n",
    "                                    p(at | had) * \\\n",
    "                                    p( coffeeshop | at)\n",
    "                                    \n",
    "Building an n-gram models computationally feasible and scalable that has made them a popular model before we had more computational power to fit in neural network models that could fit in more information and enable text generation. \n",
    "\n",
    "**RNNs and Transformers**\n",
    "\n",
    "RNNs enabled the entire context $x_{1:i-1}$ to be taken into account, which means implementing the above mentioned chain rule of probability as is, without simplifying it for n-grams. But they were again quite computationally expensive. Transformers described above reduced the computational expensive without compromising the results by having a fixed context length of \"n\" tokens, but the good thing is that you can make n large enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615091d",
   "metadata": {},
   "source": [
    "## What makes Large Language Model (LLM) different? \n",
    "\n",
    "Now that we have a clear understanding of language model, let's understand what is a \"large\" language model. They are the most advanced form of language models, trained on large amounts of data (literally the whole internet!) and use feed forward neural networks and transformers. \"Large\" can refer either to the number of parameters in the model, or sometimes the number of words in the training dataset.These capabilities have enabled LLMs to have greater language understanding, generate human-like text, answer questions, and carry out other language-related tasks. If you have played with ChatGPT, you know what I am talking about. \n",
    "\n",
    "As we have been reviewing n-grams, RNNs, transformers, we now know that earlier language models could predict the probability of a single word; modern large language models can predict the probability of sentences, paragraphs, and with the new changes announed at OpenAI Dev Day, we might be able to predict the probability of documents!\n",
    "\n",
    "<!-- They also overcome limitation of transformers based language models is them being task-specific. They require task-specific datasets and task-specific fine-tuning  -->\n",
    "\n",
    "Some very popular LLMs are:\n",
    "<li> GPT series: Large Language Models developed by OpenAI. GPT-4 is a multi-modal model which means it can work with both images and text </li>\n",
    "<li> BERT: Developed by Google, BERT is another very popular LLM </li>\n",
    "<li> XLNet: Developed by Google + CMU </li>\n",
    "<li> T5: Developed by Google </li>\n",
    "\n",
    "While LLMs sound all amazing, they are not free from limitations and ill-effects. I will be dedicating a whole chapter around limitations and opportunities of LLMs, but to summarize here, the limitations include lack of interpretability, bias, hallucination and their risk comprise loss of jobs, and creativity in humans. I will discussing everything in detail, so stay tuned. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
